name: finetune
channels:
  - conda-forge
  - nvidia           # for CUDA & cuDNN
  - pytorch
dependencies:
  # ── Python ───────────────────────────────────────────────
  - python>=3.10,<3.13

  # ── Core HF stack ────────────────────────────────────────
  - transformers>=4.41         # model wrappers & Trainer
  - datasets>=2.20             # data loading / preprocessing
  - tokenizers>=0.19           # fast BPE/ByteLevel tokenizers
  - peft>=0.11                 # LoRA / QLoRA utilities
  - accelerate>=0.30           # multi-GPU + mixed precision
  - bitsandbytes>=0.43         # 4-bit quantisation kernels
  - safetensors>=0.4           # faster/safe model weights

  # ── Performance extras (optional but recommended) ───────
  - pytorch>=2.2               # installs correct CUDA toolkit
  - torchvision                # needed by some trainer util hooks
  - flash-attn                 # Triton-based fast attention
  - ninja                      # speeds up flash-attn build
  - xformers                   # memory-efficient attention ops
  - deepspeed                  # ZeRO/Offload if you train full-precision
  - trl                        # RLHF / SFT helpers (prefix tuning, PPO)
  - einops                     # tensor reshapes in custom blocks

  # ── Dev / logging / misc ─────────────────────────────────
  - jupyterlab                 # interactive notebooks
  - scipy                      # schedulers & eval metrics
  - tqdm                       # progress bars
  - pandas                     # easy CSV / metrics handling
  - matplotlib                 # quick plots
  - tensorboard                # local dashboard
  - wandb                      # experiment tracking (log in at runtime)

  # ── Pip installs (only where conda lacks wheels) ─────────
  - pip
  - pip:
      - triton==2.2.0          # required by flash-attn if build fails
      - sentencepiece          # extra tokeniser formats
      - huggingface-hub>=0.23  # upload / download helpers
