# Configuration file for the main script
# This file contains detailed configurations for all LM modules and other required settings.
exp_name: "autoprompt_test"

distiller:
  folder: "distiller_outputs"       # Directory for distiller prompts
  api_name: "deepinfra"
  llm_config: 
    engine_name: "gpt-3.5-turbo"
    max_tokens: 200
    temperature: 2
  system_message: "You are an auto-prompting tool"
  instruction: "Please summarize the above documentation in a concise manner to describe the usage and functionality of the target"

instructor:
  engine_name: "gpt2"               # Model name for the instructor
  tokenizer: "gpt2"                 # Tokenizer name for the instructor
  template:                         # Describe the context of instruction generation
    main: "You are a sequential instruction generation tool."
    desc: "Your task is to create an instruction set as a guide for generating code. The instruction set is a sequence of instructions where each instruction adds a new layer of incremental complexity to the existing description of requirements."
    note: "DO NOT generate any code. Only generate instructions." # Add specific restrictions 
    next: "Generate the next instruction:"  # To guide LLM to generate the next instruction
  separator: "\n"                 # Separator for instructions
  max_instructions: 10              # Maximum number of instructions in a sequence
  temperature: 1.0                  # Sampling temperature for instructions
  max_len: 50                       # Maximum length of each instruction

coder:
  api_name: "deepinfra"
  system_message: "You are an expert and advanced coding tool."
  instruction: "Please generate a correct and valid code following the given task instructions."
  llm_config: 
    engine_name: "gpt-3.5-turbo"
    max_tokens: 200
    temperature: 2
    eos: ""
    max_length: 1024                  # Maximum length of generated code by the main Coder LM

# SUT configuration moved to its own top-level section for clarity
SUT:
  language: "c"                   # Programming language of the target
  path_documentation: "config/documentation/c/c_std.md"  # Path to documentation for the target
  path_example_code: null         # Path to example code (if any)
  trigger_to_generate_input: "/* Please create a advanced C program following all the instructions closely. I need the code to have good coverage for testing compiler for potential discovery of bugs.*/"  # Trigger string for generating input
  input_hint: "#include <stdlib.h>"  # Hint for input generation
  path_hand_written_prompt: null  # Optional path to a hand-written prompt component
  SUT_string: ""                  # Description of the target (e.g. API name for prompt construction)
  # New fields from SUTConfig
  timeout: 10                     # Timeout for SUT operations (e.g., validation)
  folder: "fuzz_outputs"          # Folder for SUT related outputs (e.g., temporary files, fuzz files)
  batch_size: 1                   # Batch size for code generation during SUT validation steps
  temperature: 0.8                # Temperature for code generation during SUT validation steps
  max_length: 1024                # Max length for code generation during SUT validation steps
  log_level: "INFO"               # Logging level for SUT specific logs (INFO, TRACE, VERBOSE)
  template: "fuzzing_with_config_file" # Template name for prompt creation strategy
  lambda_hyper: 0.1               # Hyperparameter lambda for reward calculation
  beta1_hyper: 1.0                # Hyperparameter beta1 for reward calculation
  special_eos: null               # Special End-Of-Sequence token for certain SUTs (e.g., SMT, GO)
  oracle_type: "opt_and_qasm"     # Oracle type for SUT validation (e.g., "crash", "diff", "metamorphic", "opt_and_qasm" for Qiskit)

fuzzer:
  # SUT block removed from here as it's now top-level
  number_of_iterations: 10
  total_time: 1
  output_folder: "logs"             # General output folder for the fuzzer (distinct from SUT.folder)
  resume: false
  otf: false
  log_level: 1                      # Logging level for the Fuzzer component (maps to INFO, TRACE, VERBOSE int enum)

trainer:
  batch_size: 1                     # Batch size for training the Coder LM
  device: "cuda"                    # Device to run the Coder LM training
  sft_ckpt: "checkpoint"
  train_steps: 1000
  grad_acc_steps: 1
  lr: 1e-4
  max_norm: 1.0
  num_warmup_steps: 10
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  buffer_size: 100
  prioritization: true
